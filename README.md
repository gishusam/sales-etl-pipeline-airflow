ğŸ›’ Sales ETL Pipeline (API â†’ PostgreSQL)

A production-style ETL (Extract, Transform, Load) pipeline that ingests product data from a public REST API, applies schema enforcement and transformations, and loads the data into a PostgreSQL (Supabase) staging table.

This project is designed as a portfolio-ready Data Engineering project, showcasing modular architecture, logging, idempotent database operations, and orchestration concepts.

ğŸš€ Project Overview
ğŸ“¡ Data Source

Public REST API:
https://fakestoreapi.com/products

ğŸ”„ Pipeline Stages

1ï¸âƒ£ Extract
Fetch raw JSON data from the API

2ï¸âƒ£ Transform
Clean, flatten, and enforce schema consistency

3ï¸âƒ£ Load
Create tables (if needed) and upsert into PostgreSQL

ğŸ§± Architecture
FakeStore API
     â†“
Extraction Layer (Python)
     â†“
Raw Data (JSON)
     â†“
Transformation Layer
     â†“
PostgreSQL (Supabase - Staging)

ğŸ“‚ Project Structure

sales-etl-pipeline/
â”‚
â”œâ”€â”€ airflow/ ğŸ› ï¸
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_ingestion_dag.py   # Airflow DAG for pipeline orchestration
â”‚
â”œâ”€â”€ ingestion/ ğŸ”„
â”‚   â”œâ”€â”€ extraction.py                 # Extract data from API
â”‚   â””â”€â”€ transformation.py             # Transform & clean data
â”‚
â”œâ”€â”€ docker/ ğŸ³
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/ ğŸ“¦
â”‚   â””â”€â”€ raw/                          # Raw JSON files (optional)
â”‚
â”œâ”€â”€ utils/ âš™ï¸
â”‚   â””â”€â”€ logger.py                     # Centralized logging
â”‚
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env                              # Environment variables
â””â”€â”€ README.md                         # Project documentation

ğŸ§  Key Features

âœ… Modular ETL design (extract / transform / load)
âœ… Apache Airflow orchestration
âœ… Centralized structured logging
âœ… Schema enforcement & data typing
âœ… Idempotent table creation
âœ… Bulk inserts with UPSERT logic
âœ… Dockerized environment

ğŸ—„ï¸ Database Schema

Table: products_staging

ğŸ—„ï¸ Database Schema

Table: `products_staging`

| Column          | Type       | Description                     |
|-----------------|-----------|---------------------------------|
| id              | INTEGER   | Product ID (Primary Key)        |
| title           | TEXT      | Product name                    |
| price           | NUMERIC   | Product price                   |
| description     | TEXT      | Product description             |
| category        | TEXT      | Product category                |
| image           | TEXT      | Product image URL               |
| rating_rate     | NUMERIC   | Average rating                  |
| rating_count    | INTEGER   | Number of ratings               |
| ingestion_date  | DATE      | Pipeline ingestion date         |
| loaded_at       | TIMESTAMP | Load timestamp                  |

âš™ï¸ Setup & Installation
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/sales-etl-pipeline.git
cd sales-etl-pipeline

2ï¸âƒ£ Create Virtual Environment
python3 -m venv venv
source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Environment Variables

Create a .env file:

POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_DB=sales_etl
DB_URL=postgresql://postgres:your_password@localhost:5432/sales_etl

ğŸ³ Running with Docker (Recommended)
Start Services
docker compose up --build
Access Airflow UI
http://localhost:8080

Default credentials:

Username: airflow
Password: airflow
Trigger DAG

DAG ID:

sales_pipeline_raw_ingestion

Click â–¶ Trigger

â–¶ï¸ Running Locally (Without Airflow)
python main.py
âœ… Expected Output
INFO - ===== ETL Pipeline Started =====
INFO - Successfully extracted 20 records
INFO - Transformation completed
INFO - Connected to database successfully
INFO - Table 'products_staging' is ready
INFO - Loaded 20 records
INFO - ===== ETL Pipeline Completed Successfully =====
ğŸ” Verifying the Data
SELECT COUNT(*) FROM products_staging;
SELECT * FROM products_staging LIMIT 5;
ğŸ§ª Possible Enhancements

ğŸ” Incremental loading strategy
ğŸ“Š Data quality checks
ğŸ§ª Unit tests
ğŸ“¦ Data warehouse layer
ğŸ“ˆ Monitoring & alerts
â˜ï¸ Cloud deployment (GCP / AWS / Azure)

ğŸ¯ Why This Project Matters

This project demonstrates:

âœ” Real-world ETL architecture
âœ” Production-style PostgreSQL loading
âœ” Data transformation & schema enforcement
âœ” Airflow orchestration
âœ” Containerized data stack

It reflects practical skills used by Data Engineers in production environments.

ğŸ‘¤ Author

Samwel Ngugi
junior Data Engineer
Python | SQL | Airflow | ETL | Docker

â­ If you found this project interesting, feel free to star the repository!
