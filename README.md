
ğŸ›’ Sales ETL Pipeline (API â†’ PostgreSQL)

A production-style ETL (Extract, Transform, Load) pipeline that ingests product data from a public REST API, applies schema enforcement and transformations, and loads the data into a PostgreSQL staging table.

This project is designed as a portfolio-ready Data Engineering project, demonstrating modular architecture, structured logging, idempotent database operations, and workflow orchestration.

 ğŸ“¡ Data Source
- Public REST API: [FakeStore API](https://fakestoreapi.com/products)

ğŸ”„ Pipeline Stages

| Stage      | Description |
|------------|-------------|
| Extract    | Fetch raw JSON data from the API |
| Transform  | Clean, flatten, and enforce schema consistency |
| Load       | Create tables if needed and upsert into PostgreSQL |



## ğŸ§± Architecture

```text
FakeStore API
     â†“
Extraction Layer (Python)
     â†“
Raw Data (JSON)
     â†“
Transformation Layer
     â†“
PostgreSQL (Supabase - Staging)
````

---

## ğŸ“‚ Project Structure

```text
sales-etl-pipeline/
â”‚
â”œâ”€â”€ airflow/ ğŸ› ï¸
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_ingestion_dag.py     # Airflow DAG for pipeline orchestration
â”‚
â”œâ”€â”€ ingestion/ ğŸ”„
â”‚   â”œâ”€â”€ extraction.py                   # Extract data from API
â”‚   â””â”€â”€ transformation.py               # Transform and clean data
â”‚
â”œâ”€â”€ docker/ ğŸ³
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/ ğŸ“¦
â”‚   â””â”€â”€ raw/                            # Raw JSON files (optional)
â”‚
â”œâ”€â”€ utils/ âš™ï¸
â”‚   â””â”€â”€ logger.py                        # Centralized logging
â”‚
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ .env                                 # Environment variables
â””â”€â”€ README.md                            # Project documentation
```

---

## ğŸ§  Key Features

* Modular ETL design (Extract / Transform / Load)
* Apache Airflow orchestration
* Centralized structured logging
* Schema enforcement and data typing
* Idempotent table creation
* Bulk inserts with UPSERT logic
* Dockerized environment

---

## ğŸ—„ï¸ Database Schema

**Table:** products_staging

| Column         | Type      | Description              |
| -------------- | --------- | ------------------------ |
| id             | INTEGER   | Product ID (Primary Key) |
| title          | TEXT      | Product name             |
| price          | NUMERIC   | Product price            |
| description    | TEXT      | Product description      |
| category       | TEXT      | Product category         |
| image          | TEXT      | Product image URL        |
| rating_rate    | NUMERIC   | Average rating           |
| rating_count   | INTEGER   | Number of ratings        |
| ingestion_date | DATE      | Pipeline ingestion date  |
| loaded_at      | TIMESTAMP | Load timestamp           |

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/sales-etl-pipeline.git
cd sales-etl-pipeline
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment Variables

Create a .env file:

```env
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_DB=sales_etl
DB_URL=postgresql://postgres:your_password@localhost:5432/sales_etl
```

---

## ğŸ³ Running with Docker (Recommended)

```bash
docker compose up --build
```

Access Airflow UI: [http://localhost:8080](http://localhost:8080)
Default credentials: airflow / airflow

Trigger DAG: sales_pipeline_raw_ingestion and click Trigger

---

## â–¶ï¸ Running Locally (Without Airflow)

```bash
python main.py
```

---

## âœ… Expected Output

```text
INFO - ===== ETL Pipeline Started =====
INFO - Successfully extracted 20 records
INFO - Transformation completed
INFO - Connected to database successfully
INFO - Table 'products_staging' is ready
INFO - Loaded 20 records
INFO - ===== ETL Pipeline Completed Successfully =====
```

---

## ğŸ” Verifying the Data

```sql
SELECT COUNT(*) FROM products_staging;
SELECT * FROM products_staging LIMIT 5;
```

---

## ğŸ§ª Possible Enhancements

* Incremental loading strategy
* Data quality checks
* Unit and integration tests
* Data warehouse layer
* Monitoring and alerts
* Cloud deployment (GCP / AWS / Azure)

---

## ğŸ¯ Why This Project Matters

This project demonstrates:

* Real-world ETL architecture
* Production-style PostgreSQL loading
* Data transformation and schema enforcement
* Airflow orchestration
* Containerized data stack

It reflects practical skills used by Data Engineers in production environments.

---

## ğŸ‘¤ Author

Samwel Ngugi â€“ Junior Data Engineer
Python | SQL | Airflow | ETL | Docker

If you found this project interesting, feel free to star the repository!


