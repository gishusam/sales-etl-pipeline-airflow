ğŸ“Š Sales ETL Pipeline with Airflow

An end-to-end data engineering pipeline that extracts product data from an API, stores raw data in cloud storage, transforms it, and loads it into a PostgreSQL (Supabase) staging table using Apache Airflow, Docker, and Python.

ğŸš€ Project Overview

This project demonstrates a modern production-style ETL pipeline:

Extract â€“ Pull product data from FakeStore API

Load (Raw) â€“ Store raw JSON data

Transform â€“ Clean & flatten nested fields

Load (Staging) â€“ Upsert into PostgreSQL (Supabase)

Orchestration â€“ Managed via Apache Airflow

Containerization â€“ Fully Dockerized

ğŸ— Architecture

API â†’ Airflow DAG â†’ Raw Storage â†’ Transformation â†’ Supabase (Postgres)

ğŸ§° Tech Stack

Python

Apache Airflow

Docker & Docker Compose

PostgreSQL (Supabase)

FakeStore API

psycopg2

Requests

ğŸ“‚ Project Structure
sales-etl-pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_ingestion_dag.py
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ extraction.py
â”‚   â””â”€â”€ transformation.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
âš™ï¸ Features

âœ” API data ingestion
âœ” Raw data persistence
âœ” Schema-on-write staging layer
âœ” Idempotent loads (ON CONFLICT upsert)
âœ” Airflow orchestration
âœ” Dockerized environment

ğŸ”„ Pipeline Workflow
1ï¸âƒ£ Extraction

Calls FakeStore API

Saves raw JSON file

Returns file path to Airflow

2ï¸âƒ£ Transformation

Flattens nested rating fields

Adds metadata:

ingestion_date

loaded_at

3ï¸âƒ£ Load to Staging

Ensures table exists

Bulk inserts using execute_values

Upserts on conflict

ğŸ³ Running the Project
1ï¸âƒ£ Start Services
docker compose up --build
2ï¸âƒ£ Access Airflow UI

Open:

http://localhost:8080

Default credentials:

Username: airflow
Password: airflow
3ï¸âƒ£ Trigger DAG

DAG Name:

sales_pipeline_raw_ingestion

Click â–¶ Trigger

ğŸ—„ Database (Supabase)

Pipeline loads into:

products_staging

Schema includes:

id

title

price

description

category

image

rating_rate

rating_count

ingestion_date

loaded_at

ğŸ” Environment Variables

Sensitive configs stored in .env:

POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_DB=
DB_URL=
ğŸ“ˆ Production Concepts Demonstrated

âœ… Data Lake (Raw layer)
âœ… Staging Layer
âœ… Idempotent Loads
âœ… Metadata Columns
âœ… Orchestration
âœ… Containerized Infra

ğŸ¯ Learning Objectives

This project was built to practice:

Airflow DAG design

Dockerized pipelines

API ingestion

Data transformation

PostgreSQL loading

Production ETL patterns

ğŸš§ Future Improvements

â¬œ Data quality checks
â¬œ Warehouse layer
â¬œ Incremental loads
â¬œ Partitioning strategy
â¬œ Monitoring & alerts
â¬œ CI/CD pipeline

ğŸ‘¨â€ğŸ’» Author

Samwel Ngugi

Aspiring Data Engineer
Focused on building production-grade data pipelines
