ğŸ“Š Sales ETL Pipeline with Apache Airflow

An end-to-end data engineering pipeline that extracts product data from an external API, stores raw data, transforms it into an analytics-ready structure, and loads it into a PostgreSQL (Supabase) staging table â€” orchestrated using Apache Airflow and fully containerized with Docker.

ğŸš€ Project Overview

This project simulates a production-style ETL workflow:

Extract â€“ Retrieve product data from FakeStore API
Load (Raw) â€“ Persist raw JSON data
Transform â€“ Clean, flatten, and enrich data
Load (Staging) â€“ Upsert into PostgreSQL (Supabase)
Orchestration â€“ Managed via Apache Airflow
Infrastructure â€“ Dockerized services

ğŸ— Architecture
FakeStore API
     â†“
Airflow DAG
     â†“
Raw Data Storage (JSON)
     â†“
Transformation Layer
     â†“
Supabase (PostgreSQL - Staging)

ğŸ§° Tech Stack

Python
Apache Airflow
Docker & Docker Compose
PostgreSQL (Supabase)
FakeStore API
psycopg2
Requests

ğŸ“‚ Project Structure
sales-etl-pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_ingestion_dag.py
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ extraction.py
â”‚   â””â”€â”€ transformation.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md


â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
âš™ï¸ Key Features

âœ” API data ingestion
âœ” Raw data persistence
âœ” Data transformation & enrichment
âœ” PostgreSQL staging layer
âœ” Idempotent loads (Upserts)
âœ” Airflow orchestration
âœ” Dockerized environment

ğŸ”„ Pipeline Workflow
1ï¸âƒ£ Extraction

Calls FakeStore API

Retrieves product data (JSON)

Stores raw dataset

Passes file path to downstream tasks

2ï¸âƒ£ Transformation

Flattens nested fields (e.g., ratings)

Standardizes schema

Adds metadata columns:

ingestion_date

loaded_at

3ï¸âƒ£ Load to Staging

Ensures table exists

Performs bulk insert

Uses ON CONFLICT for safe upserts

ğŸ³ Running the Project
1ï¸âƒ£ Start Services
docker compose up --build
2ï¸âƒ£ Access Airflow UI

Open in browser:

http://localhost:8080

Default credentials:

Username: airflow
Password: airflow
3ï¸âƒ£ Trigger the DAG

DAG Name:

sales_pipeline_raw_ingestion

Click â–¶ Trigger

ğŸ—„ Database (Supabase / PostgreSQL)

Data is loaded into:

products_staging
Schema

  id
  title 
  price
  description
  category
  image
  rating_rate
  rating_count
  ingestion_date
  loaded_at

ğŸ” Environment Variables

Sensitive credentials are stored in .env:

POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_DB=
DB_URL=
ğŸ“ˆ Production Concepts Demonstrated

âœ… Data Lake (Raw Layer)
âœ… Staging Layer
âœ… Idempotent Loads
âœ… Schema-on-write
âœ… Metadata Tracking
âœ… Workflow Orchestration
âœ… Containerized Infrastructure

ğŸ¯ Learning Objectives

This project was built to strengthen skills in:

Apache Airflow DAG design
Dockerized data pipelines
API ingestion patterns
Data transformation
PostgreSQL loading
Production ETL best practices

ğŸš§ Future Enhancements

â¬œ Data quality checks
â¬œ Data warehouse layer
â¬œ Incremental loading strategy
â¬œ Partitioning & indexing
â¬œ Monitoring & alerting
â¬œ CI/CD pipeline

ğŸ‘¨â€ğŸ’» Author

Samwel Ngugi
Junior Data Engineer

Focused on designing and building production-grade data pipelines
